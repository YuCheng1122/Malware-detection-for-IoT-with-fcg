import os
import json
import re
import logging
import numpy as np
from gensim.models import Word2Vec
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
import gc

class Word2VecCBOW:
    def __init__(self, word2vec_params, custom_params):
        self.word2vec_params = word2vec_params
        self.custom_params = custom_params
        self.model = None
        self.save_path = custom_params['save_path']

    def reload_data(self):
        data_path = f"{self.save_path}_processed_data.json"
        with open(data_path, 'r') as f:
            for line in f:
                yield json.loads(line)

    def parse_fcg_json_file(self, file_path):
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            instructions = [instr for func in data.values() for instr in func.get('instructions', [])]
            logging.debug(f"Parsed {len(instructions)} instructions from {file_path}.")
            return [re.findall(r'\w+', instruction) for instruction in instructions]
        except Exception as e:
            logging.error(f"Error processing file {file_path}: {e}")
            return []

    def process_data(self, file_paths):
        processed_data_path = f"{self.save_path}_processed_data.json"
        
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(processed_data_path), exist_ok=True)
        
        def process_batch(batch):
            with ProcessPoolExecutor(max_workers=self.word2vec_params['workers']) as executor:
                futures = [executor.submit(self.parse_fcg_json_file, file_path) for file_path in batch]
                return [result for future in tqdm(as_completed(futures), total=len(batch), desc="Processing files", unit="file") for result in future.result()]

        with open(processed_data_path, 'w') as f:
            for i in tqdm(range(0, len(file_paths), self.custom_params['batch_size']), desc="Processing batches", unit="batch"):
                batch = file_paths[i:i + self.custom_params['batch_size']]
                logging.info(f"Processing batch {i // self.custom_params['batch_size'] + 1} of {len(file_paths) // self.custom_params['batch_size'] + 1}")
                batch_results = process_batch(batch)
                for result in batch_results:
                    f.write(json.dumps(result) + '\n')
                del batch_results
                gc.collect()

        logging.info(f"Processed data saved to {processed_data_path}")

    def build_vocab(self, sentences):
        self.model = Word2Vec(**self.word2vec_params)
        self.model.build_vocab(sentences)

    def train(self, file_paths):
        processed_data_path = f"{self.save_path}_processed_data.json"
        if not os.path.exists(processed_data_path):
            self.process_data(file_paths)
        
        # Load all sentences
        sentences = list(self.reload_data())
        
        # Build vocabulary
        logging.info("Building vocabulary...")
        self.build_vocab(sentences)
        
        for epoch in range(self.custom_params['epochs']):
            logging.info(f"Starting epoch {epoch + 1}/{self.custom_params['epochs']}")
            self.model.train(sentences, total_examples=len(sentences), epochs=1)
            self.save_model(f"{self.save_path}_epoch_{epoch + 1}")
            logging.info(f"Epoch {epoch + 1}/{self.custom_params['epochs']} completed")
            gc.collect()

    def save_model(self, path=None):
        if path is None:
            path = self.save_path
        os.makedirs(os.path.dirname(path), exist_ok=True)
        self.model.save(path)
        logging.info(f"Model saved to {path}")

    def load_model(self, path=None):
        if path is None:
            path = self.save_path
        if os.path.exists(path):
            self.model = Word2Vec.load(path)
            logging.info(f"Model loaded from {path}")
        else:
            raise FileNotFoundError(f"No saved model found at {path}")

    def vectorize_instruction(self, instruction):
        words = re.findall(r'\w+', instruction)
        vectors = [self.model.wv[word] for word in words if word in self.model.wv]
        if vectors:
            return np.mean(vectors, axis=0)
        else:
            return np.zeros(self.word2vec_params['vector_size'])