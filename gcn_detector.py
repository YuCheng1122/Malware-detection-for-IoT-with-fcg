import numpy as np
import torch
import logging
import os
import json
import networkx as nx
from typing import Any, List, Tuple, Dict
from tqdm import tqdm
from torch_geometric.data import Data
from torch_geometric.utils import from_networkx
from torch_geometric.loader import DataLoader
from sklearn.model_selection import train_test_split
from utils.normalize_assembly import NormalizeAssembly
from utils.word2vec_cbow import Word2VecCBOW
from utils.graph_preprocessing import GraphProcessor
from models.gcn_attention import GCNWithAttention
from models.train_gcn import train_gcn_model 
from models.predict import Predictor
from utils.tools import load_dataset_from_directory, load_datasets, create_data_loader, get_param, create_data_loader
from sklearn.model_selection import train_test_split
from malwareDetector.detector import detector
from types import SimpleNamespace

class SubDetector(detector):
    
    def __init__(self, config, loggers=None):
        self.config = config
        self.check_environment()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.loggers = loggers or {k: logging.getLogger(k) for k in ['gcn', 'word2vec', 'graph_processing', 'assembly_normalization']}
        self.normalizer = NormalizeAssembly(self.config, self.loggers['assembly_normalization'])
        self.word2vec_model = None
        self.graph_processor = None
        self.gcn_model = None
        self.predictor = Predictor(self.config, self.loggers['gcn'], self.device)
        self.mkdir()
        self.extracted_features = None
    
    def check_environment(self):
        """Check if the necessary directories and files exist"""
        if not os.path.exists(self.config.folder.DATASET_DIR):
            raise FileNotFoundError("Dataset directory not found. Please create a Dataset directory in the project root.")
        
        # Check for benign and malware directories
        for subdir in ["benign", "malware"]:
            if not os.path.exists(os.path.join(self.config.folder.DATASET_DIR, subdir)):
                raise FileNotFoundError(f"{subdir.capitalize()} directory not found in Dataset. Please create a {subdir} directory.")
        
        # Check for JSON, DOT, or pickle files
        file_types = [".json", ".dot", ".pickle"]
        files_found = any(
            any(f.endswith(ext) for ext in file_types)
            for root, _, files in os.walk(self.config.folder.DATASET_DIR)
            for f in files
        )
        if not files_found:
            raise FileNotFoundError("No JSON, DOT, or pickle files found in the Dataset directory or its subdirectories.")
        
    def mkdir(self):
        """Create necessary directories"""
        folders = [
            self.config.path.NORMALIZED_OUT_PATH,
            self.config.path.WORD2VEC_OUT_PATH,
            self.config.path.GRAPH_OUT_PATH,
            self.config.path.GCN_MODEL_PATH,
            self.config.path.LOGS_PATH,
            self.config.folder.DATASET_DIR,
            self.config.folder.PREDICT_DIR
        ]
        for folder in folders:
            os.makedirs(folder, exist_ok=True)

    def normalize_assembly_files(self):
        self.loggers['assembly_normalization'].info("Normalizing assembly files...")
        self.normalizer.normalize_training_data()
    
    def normalize_prediction_files(self):
        self.loggers['assembly_normalization'].info("Normalizing prediction files...")
        self.normalizer.normalize_prediction_data()
        
    def train_word2vec(self):
        self.loggers['word2vec'].info("Training Word2Vec model...")
        self.word2vec_model = Word2VecCBOW(self.config, self.loggers['word2vec'])
        try:
            self.word2vec_model.run()
            self.loggers['word2vec'].info(f"Word2Vec training completed")
            final_model_path = self.word2vec_model.save_path
            self.word2vec_model.model.save(final_model_path)
            self.loggers['word2vec'].info(f"Final Word2Vec model saved to {final_model_path}")
        except Exception as e:
            self.loggers['word2vec'].error(f"Error during Word2Vec training: {e}")
            raise

    def process_graphs(self):
        self.loggers['graph_processing'].info("Processing graphs...")
        self.graph_processor = GraphProcessor(self.config, self.loggers['graph_processing'])
        self.graph_processor.process_files()
    
    def train_gcn(self):
        if self.gcn_model is None:
            self._initialize_model()
        
        self.loggers['gcn'].info("Starting GCN training")
        self.loggers['gcn'].debug(f"Configuration: {json.dumps(self.config.__dict__, indent=2, default=lambda o: o.__dict__)}")

        gcn_params = self.config.model.gcn_params

        dataset = load_datasets(self.config)
        train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)
        train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)
        
        train_loader = create_data_loader(train_data, gcn_params.batch_size)
        val_loader = create_data_loader(val_data, gcn_params.batch_size, shuffle=False)
        test_loader = create_data_loader(test_data, gcn_params.batch_size, shuffle=False)
        
        model_save_path = os.path.join(self.config.path.GCN_MODEL_PATH, gcn_params.save_path)
        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)

        final_accuracy, final_f1 = train_gcn_model(
            model=self.gcn_model,
            train_loader=train_loader,
            val_loader=val_loader,
            test_loader=test_loader,
            learning_rate=gcn_params.learning_rate,
            epochs=gcn_params.epochs,
            batch_size=gcn_params.batch_size,
            model_save_path=model_save_path,
            device=self.device,
            logger=self.loggers['gcn']
        )

        self.loggers['gcn'].info(f"GCN training completed. Final Accuracy: {final_accuracy:.4f}, Final F1: {final_f1:.4f}")
        return final_accuracy, final_f1
    
    def predict(self, target_evasion_rate: float = 0.5):
        self.loggers['gcn'].info("Starting prediction process")
        
        try:
            all_preds, all_probs, all_files, results = self.predictor.predict(target_evasion_rate)
            self.loggers['gcn'].info("Prediction process completed successfully")
            return all_preds, all_probs, all_files, results
        except Exception as e:
            self.loggers['gcn'].error(f"Error in prediction process: {str(e)}")
            raise

    def _initialize_model(self):
        self.loggers['gcn'].debug(f"Initializing GCN model with params: {self.config.model.gcn_params}")
        self.gcn_model = GCNWithAttention(self.config).to(self.device)
