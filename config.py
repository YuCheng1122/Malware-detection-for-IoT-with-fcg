from typing import Optional
from pydantic import BaseModel, Extra
import json

class PathConfig(BaseModel, extra=Extra.allow):
    DEFAULT_INPUT_PATH: str = "./Dataset/"
    DEFAULT_OUT_PATH: str = "./NormalizedDataset/"
    DEFAULT_CONFIG_PATH: str = "./config.json"

class FolderConfig(BaseModel, extra=Extra.allow):
    DATASET_DIR: str = "./Dataset/"
    FEATURE_DIR: str = "./Feature/"
    VECTORIZE_DIR: str = "./Vectorize/"
    MODEL_DIR: str = "./Model/"
    PREDICT_DIR: str = "./Predict/"

class Word2VecParams(BaseModel):
    vector_size: int = 300
    window: int = 5
    min_count: int = 1
    max_vocab_size: Optional[int] = None  # Changed this line to allow None
    sample: float = 0.001
    alpha: float = 0.001
    min_alpha: float = 0.0001
    workers: int = 4
    hs: int = 0
    negative: int = 5
    sg: int = 0
    cbow_mean: int = 1
    sorted_vocab: int = 1
    compute_loss: bool = True
    ns_exponent: float = 0.75
    batch_words: int = 10000

class CustomParams(BaseModel):
    epochs: int = 10
    save_path: str = "./model_saved/word2vec_final"
    batch_size: int = 1024

class GCNParams(BaseModel):
    num_features: int = 300
    hidden_channels: int = 64
    num_classes: int = 2
    learning_rate: float = 0.001
    epochs: int = 70
    batch_size: int = 250
    save_path: str = "./model_saved/sage_with_attention_graph_val3.pth"
    dropout_rate: float = 0.5
    num_rounds: int = 2
    num_layers: int = 2
    function_embedding_dim: int = 64
    max_nodes: int = 200

class ModelConfig(BaseModel, extra=Extra.allow):
    word2vec_params: Word2VecParams = Word2VecParams()
    custom_params: CustomParams = CustomParams()
    gcn_params: GCNParams = GCNParams()

class Config(BaseModel):
    path: PathConfig = PathConfig()
    folder: FolderConfig = FolderConfig()
    model: ModelConfig = ModelConfig()

def read_config(config_file_path: str) -> Config:
    with open(config_file_path, 'r') as f:
        config_data = json.load(f)
    return Config(**config_data)