import torch
import torch.nn.functional as F
from torch.nn import Linear, BatchNorm1d
from torch_geometric.nn import SAGEConv, global_mean_pool

class GCNWithAttention(torch.nn.Module):

    def __init__(self, config):
        """
        Initialize the GCN with Attention model using parameters from a config object.
        
        Args:
            config: Configuration object containing model parameters.
        """
        super(GCNWithAttention, self).__init__()
        
        gcn_params = config.model.gcn_params
        
        num_features = gcn_params.num_features
        hidden_channels = gcn_params.hidden_channels
        num_rounds = gcn_params.num_rounds
        num_layers = gcn_params.num_layers
        function_embedding_dim = gcn_params.function_embedding_dim
        self.dropout_rate = gcn_params.dropout_rate

        self.function_embedding = Linear(num_features, function_embedding_dim)
        self.convs = torch.nn.ModuleList()
        self.batch_norms = torch.nn.ModuleList()
        for _ in range(num_rounds):
            for _ in range(num_layers):
                self.convs.append(SAGEConv(function_embedding_dim, hidden_channels))
                self.batch_norms.append(BatchNorm1d(hidden_channels))
        self.lin1 = Linear(hidden_channels, 128)
        self.lin2 = Linear(128, 128)
        self.attention = Linear(hidden_channels, 1)
        self.lin = Linear(128, 1)

    def forward(self, data):
        """
        Forward pass of the GCN with Attention model.
        
        Args:
            data: PyTorch Geometric data object containing x, edge_index, and batch.
        
        Returns:
            torch.Tensor: Output predictions.
        """
        x, edge_index, batch = data.x, data.edge_index, data.batch
        x = self.function_embedding(x)
        for conv, bn in zip(self.convs, self.batch_norms):
            x = conv(x, edge_index)
            x = bn(x)
            x = F.relu(x)
        attn_weights = F.leaky_relu(self.attention(x))
        attn_weights = F.softmax(attn_weights, dim=0)
        x = x * attn_weights
        x = global_mean_pool(x, batch)
        x = F.relu(self.lin1(x))
        x = F.relu(self.lin2(x))
        x = F.dropout(x, p=self.dropout_rate, training=self.training)
        x = self.lin(x)
        return torch.sigmoid(x)